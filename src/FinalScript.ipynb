{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "import os\n",
    "import time\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Linear models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "#Ensemble models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "#Other models\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "#Sklearn\n",
    "from sklearn.datasets import _california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#Settings\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables\n",
    "\n",
    "#Creation du dataframe avec le dataset\n",
    "dataset = _california_housing.fetch_california_housing(as_frame=True)\n",
    "df = dataset.frame\n",
    "\n",
    "#Definition de la target\n",
    "target_name = \"MedHouseVal\"\n",
    "target = df[target_name]\n",
    "\n",
    "#Colonnes exclues pour le test\n",
    "columns_to_drop = [\n",
    "    target_name \n",
    "]\n",
    "data = df.drop(columns=columns_to_drop)\n",
    "\n",
    "#Split\n",
    "data_train, data_test, target_train, target_test = train_test_split(data, target, test_size=0.2, random_state=69)\n",
    "\n",
    "#Variables globales\n",
    "global_random_state = 69\n",
    "\n",
    "#Modèles à tester\n",
    "linear_models = [\n",
    "    LinearRegression(),\n",
    "    Ridge(random_state=global_random_state),\n",
    "    RidgeCV(),\n",
    "    Lasso(random_state=global_random_state),\n",
    "]\n",
    "\n",
    "ens_models = [\n",
    "    AdaBoostRegressor(random_state=global_random_state),  \n",
    "    BaggingRegressor(random_state=global_random_state, n_jobs=-1), \n",
    "    ExtraTreesRegressor(random_state=global_random_state, n_jobs=-1), \n",
    "    GradientBoostingRegressor(random_state=global_random_state), \n",
    "    RandomForestRegressor(random_state=global_random_state, n_jobs=-1), \n",
    "    HistGradientBoostingRegressor(random_state=global_random_state)\n",
    "]\n",
    "\n",
    "boost_and_bag_models = [\n",
    "    AdaBoostRegressor(base_estimator=BaggingRegressor(random_state=global_random_state, n_jobs=-1)),\n",
    "    AdaBoostRegressor(base_estimator=ExtraTreesRegressor(random_state=global_random_state, n_jobs=-1)),\n",
    "    AdaBoostRegressor(base_estimator=GradientBoostingRegressor(random_state=global_random_state)),\n",
    "    AdaBoostRegressor(base_estimator=RandomForestRegressor(random_state=global_random_state, n_jobs=-1)),\n",
    "    AdaBoostRegressor(base_estimator=HistGradientBoostingRegressor(random_state=global_random_state)),\n",
    "\n",
    "    BaggingRegressor(base_estimator=BaggingRegressor(random_state=global_random_state, n_jobs=-1)),\n",
    "    BaggingRegressor(base_estimator=ExtraTreesRegressor(random_state=global_random_state, n_jobs=-1)),\n",
    "    BaggingRegressor(base_estimator=GradientBoostingRegressor(random_state=global_random_state)),\n",
    "    BaggingRegressor(base_estimator=RandomForestRegressor(random_state=global_random_state, n_jobs=-1)),\n",
    "    BaggingRegressor(base_estimator=HistGradientBoostingRegressor(random_state=global_random_state)),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction qui retourne le score de chaque modèle d'un array\n",
    "def evaluate_models(models):\n",
    "    \n",
    "    #Initialisation de la dataframe des resultats\n",
    "    result_df = pd.DataFrame(columns=[\"ModelName\",\"Score\",\"ExecTime\"])\n",
    "\n",
    "    for model in models:\n",
    "\n",
    "        #Demarrage du timer\n",
    "        start = time.time()\n",
    "\n",
    "        #Fit\n",
    "        model.fit(data_train, target_train)\n",
    "\n",
    "        #Calcul du score\n",
    "        score = model.score(data_test, target_test)\n",
    "\n",
    "        #Sauvegarde des stats dans la dataframe\n",
    "        model_name = model.__class__.__name__\n",
    "        new_row = pd.DataFrame([[model_name, score *100, time.time() - start]], columns=[\"ModelName\",\"Score\",\"ExecTime\"])\n",
    "        result_df = pd.concat([result_df, new_row])\n",
    "\n",
    "    #Affichage\n",
    "    print(f\"-\"*36, \"\\nResults :\")\n",
    "    print(result_df.to_string(index=False))\n",
    "    print(f\"-\"*36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonctions pour afficher les mse & rmse d'un modèle d'un array en fonction du training set size\n",
    "\n",
    "#Affiche chaque courbe sur le même plot\n",
    "def draw_curve(mse_values, rmse_values, y_lim_min, y_lim_max, model_name, graph):\n",
    "\n",
    "    train_size, mse_train_scores_mean, mse_test_scores_mean = mse_values\n",
    "    train_size, rmse_train_scores_mean, rmse_test_scores_mean = rmse_values\n",
    "\n",
    "    graph.set_title(model_name)\n",
    "    graph.set_xlabel(\"Training set size\")\n",
    "    graph.set_ylabel(\"Error\")\n",
    "\n",
    "    graph.set_ylim(y_lim_min,y_lim_max)\n",
    "    graph.plot(train_size, mse_train_scores_mean)\n",
    "    graph.plot(train_size, mse_test_scores_mean)\n",
    "    graph.plot(train_size, rmse_train_scores_mean)\n",
    "    graph.plot(train_size, rmse_test_scores_mean)\n",
    "    graph.legend(labels=[\"train_mse\", \"test_mse\", \"train_rmse\", \"test_rmse\"], loc=\"upper right\")\n",
    "\n",
    "#Calcule les points de chaque courbe\n",
    "def compute_learning_curve(model, scoring, point_amount):\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "                                                    random_state=global_random_state,\n",
    "                                                    estimator = model,\n",
    "                                                    X=data,\n",
    "                                                    y=target,\n",
    "                                                    scoring=scoring,\n",
    "                                                    n_jobs=-1,\n",
    "                                                    train_sizes=np.linspace(0.1, 1, point_amount)\n",
    "                                                    )\n",
    "    return train_sizes, -np.mean(train_scores, axis=1), -np.mean(test_scores, axis=1)\n",
    "\n",
    "#Fonction initiale\n",
    "def show_learning_curves(models, y_lim_min, y_lim_max, point_amount):\n",
    "    \n",
    "    #Creation du subplots\n",
    "    plt.figure(1)\n",
    "    fig, graphs = plt.subplots(1, len(models), figsize=(5* len(models), 5))\n",
    "\n",
    "    #On boucle sur chaque modèle de l'array\n",
    "    for i in tqdm(range(len(models))):\n",
    "        draw_curve(\n",
    "            compute_learning_curve(models[i], \"neg_mean_squared_error\", point_amount),\n",
    "            compute_learning_curve(models[i], \"neg_root_mean_squared_error\", point_amount),\n",
    "            y_lim_min,\n",
    "            y_lim_max,\n",
    "            models[i].__class__.__name__, \n",
    "            graphs[i]\n",
    "            )\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction qui fit et score\n",
    "def fit_and_score(alg):\n",
    "    alg.fit(data_train, target_train)\n",
    "    result_score = alg.score(data_test, target_test) * 100\n",
    "    print(\"R2 : \" + str(result_score))\n",
    "    #return result_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction qui calcule l'erreur\n",
    "def evaluate_error(model):\n",
    "    r = np.absolute(model.predict(data) - target)*100000\n",
    "    print(str(np.mean(r)) + \" +- \" + str(np.std(r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction qui sauvegarde l'IA entrainée\n",
    "\n",
    "def save_ai(model, file_name):\n",
    "\n",
    "    with open(f\"Saved AI/{file_name}.joblib\", \"wb\") as fo:\n",
    "        joblib.dump(model, fo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction qui charge l'IA entrainée\n",
    "\n",
    "def load_ai(file_name):\n",
    "    with open(f\"Saved AI/{file_name}.joblib\", \"rb\") as fo:\n",
    "        return joblib.load(fo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - Début de l'analyse des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Premiers essais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomForestClassifier\n",
    "\n",
    "modele_rf = RandomForestClassifier(\n",
    "    # il s’agit du nombre d’arbres dans la forêt\n",
    "    n_estimators=5,\n",
    "    # il s’agit du critère utilisé pour construire les arbres et séparer les branches des arbres\n",
    "    criterion='gini',\n",
    "    # il s’agit de la profondeur maximale des arbres utilisés (le nombre de niveaux dans l’arbre de décision)\n",
    "    max_depth=None,\n",
    "    # il s’agit du nombre d’échantillons minimal dans une feuille pour refaire une séparation\n",
    "    min_samples_split=2,\n",
    "    # il s’agit du nombre d’échantillons minimal pour créer une feuille\n",
    "    min_samples_leaf=1,\n",
    "    # il s’agit de la fraction du nombre total d’échantillon minimal pour créer une feuille\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    # il s’agit du nombre maximal de feuilles\n",
    "    max_leaf_nodes=None,\n",
    "    # il s’agit de la baisse minimale du critère d’impureté pour faire une séparation\n",
    "    min_impurity_decrease=0.0,\n",
    "    # paramètre pour utiliser du bootstrap, si il est à False, le même échantillon est pris pour chaque arbre\n",
    "    bootstrap=True,\n",
    "    # ??\n",
    "    oob_score=False,\n",
    "    # nombre de traitements à effectuer en parallèle\n",
    "    n_jobs=None,\n",
    "    # graine aléatoire\n",
    "    random_state=None,\n",
    "    # ??\n",
    "    verbose=0,\n",
    "    # ceci permet de repartir du résultat du dernier apprentissage pour faire l’apprentissage\n",
    "    warm_start=False,\n",
    "    # il s’agit des poids associés à chaque classe si cela a un sens\n",
    "    class_weight=None,\n",
    "    # ??\n",
    "    ccp_alpha=0.0,\n",
    "    # si vous voulez réduire le nombre d’observations dans vos échantillons bootstrap\n",
    "    max_samples=None,\n",
    ")\n",
    "\n",
    "#modele_rf.fit(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNeighborsRegressor\n",
    "\n",
    "#Colonnes exclues pour le train\n",
    "KNeighborsRegressor_data = df.drop(columns=[target_name, \"AveOccup\", \"Population\", \"HouseAge\", \"AveBedrms\" , \"AveRooms\"])\n",
    "\n",
    "#Entrainement\n",
    "KNeighborsRegressor_model = KNeighborsRegressor()\n",
    "KNeighborsRegressor_model.fit(KNeighborsRegressor_data, target)\n",
    "\n",
    "target_predicted = KNeighborsRegressor_model.predict(KNeighborsRegressor_data)\n",
    "print(target[:5])\n",
    "print(target_predicted[:5])\n",
    "print(f\"Number of correct prediction: \"\n",
    "      f\"{(target[:5] == target_predicted[:5]).sum()} / 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Modèles linéaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test de tous les modèles linéaires\n",
    "\n",
    "evaluate_models(linear_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSE et RMSE de tous les modèles linéaires\n",
    "\n",
    "show_learning_curves(linear_models, y_lim_min=-0.5, y_lim_max=2, point_amount=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test de tous les modèles ensemblistes\n",
    "\n",
    "evaluate_models(ens_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSE et RMSE de tous les modèles ensemblistes\n",
    "\n",
    "show_learning_curves(ens_models, y_lim_min=0, y_lim_max=2, point_amount=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III - Recherche des meilleurs hyperparamètres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Optimisation du HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Score du modèle de base\n",
    "\n",
    "base_model = HistGradientBoostingRegressor(\n",
    "    random_state=global_random_state,\n",
    ")\n",
    "\n",
    "fit_and_score(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recherche de \"max_iter\", \"max_depth\", \"learning_rate\", \"min_samples_leaf\", \"max_depth\"\n",
    "\n",
    "model = HistGradientBoostingRegressor(\n",
    "    random_state=global_random_state\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    \"max_iter\":range(10,401,10),\n",
    "    \"max_depth\":range(5,31,2),\n",
    "    \"learning_rate\":[0.1,1],\n",
    "    \"min_samples_leaf\":range(10, 121, 10),\n",
    "    \"max_leaf_nodes\":range(20,41,2)\n",
    "}\n",
    "\n",
    "best_model = RandomizedSearchCV(estimator=model, param_distributions=parameters, n_iter=2000, n_jobs=-1)\n",
    "\n",
    "#Fit\n",
    "fit_and_score(best_model)\n",
    "\n",
    "#Affichage de l'erreur\n",
    "evaluate_error(best_model.best_estimator_)\n",
    "\n",
    "#Stockage pour l'AdaBoostRegressor\n",
    "best_hist = best_model.best_estimator_\n",
    "\n",
    "#Affichage\n",
    "print(best_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Optimisation du AdaBoostRegressor(HistGradientBoostingRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Score de base de la combinaison de modèles\n",
    "\n",
    "base_boosted_model = AdaBoostRegressor(\n",
    "    random_state=global_random_state, \n",
    "    base_estimator=best_hist\n",
    ")\n",
    "\n",
    "#Fit\n",
    "fit_and_score(base_boosted_model)\n",
    "\n",
    "#Affichage de l'erreur\n",
    "evaluate_error(base_boosted_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recherche \"n_estimators\" & \"learning_rate\"\n",
    "boosted_model = AdaBoostRegressor(random_state=global_random_state,\n",
    "                                    base_estimator=best_hist,\n",
    "                                    )\n",
    "\n",
    "parameters = {\n",
    "    \"n_estimators\":range(20, 200, 10),\n",
    "    \"learning_rate\":[0.1, 1]\n",
    "}\n",
    "\n",
    "best_boosted_model = RandomizedSearchCV(estimator=boosted_model, param_distributions=parameters, n_iter=30, n_jobs=-1)\n",
    "\n",
    "#Fit\n",
    "best_boosted_model.fit(data_train, target_train)\n",
    "\n",
    "#Affichage du score\n",
    "final_score = best_boosted_model.score(data_test, target_test) * 100\n",
    "print(\"R2 : \" + str(final_score))\n",
    "\n",
    "#Affichage de l'erreur\n",
    "evaluate_error(best_boosted_model.best_estimator_)\n",
    "\n",
    "#Affichage\n",
    "print(best_boosted_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV - Sauvegarde de l'IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sauvegarde\n",
    "\n",
    "save_ai(best_boosted_model, file_name=str(final_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V - Test de l'IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "\n",
    "#Chargement de l'IA sauvegardée\n",
    "ai = load_ai(str(final_score))\n",
    "\n",
    "#Predict\n",
    "predicted = ai.predict(data)\n",
    "d = {'Predicted': predicted, 'Original': target, 'Ecart': np.absolute(predicted-target)*100000}\n",
    "\n",
    "#Stockage et affichage du resultat\n",
    "results_dataframe = pd.DataFrame(data=d)\n",
    "print(results_dataframe)\n",
    "\n",
    "#Affichage des stats\n",
    "print(np.min(results_dataframe[\"Ecart\"]))\n",
    "print(np.average(results_dataframe[\"Ecart\"]))\n",
    "print(np.median(results_dataframe[\"Ecart\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9cca3b5787ed073d3b77f03b758985ef55479ddb50c5b8f16161c9ada7875b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
